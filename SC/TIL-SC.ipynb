{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TIL-SC.ipynb","provenance":[],"collapsed_sections":["CtLPLbA0cxBE","t9z4qZ8A8HTn","Tz-eN4RZTvyW","5CD5HBKjCA3m","E3_rz0d4Khpn","Q9hhlhDDMEos","3OWoxpWcM3D4","rUGlO1lFRQQd","qeGHrPzIa9_7"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CtLPLbA0cxBE"},"source":["## Downloading Dependencies"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2h7Cxnb-5XHg","executionInfo":{"status":"ok","timestamp":1624433417014,"user_tz":-480,"elapsed":3826,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}},"outputId":"ff9a6cd9-69fc-4a2b-ae36-0881eccc00c0"},"source":["# install torchaudio\n","!pip install torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torchaudio==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n","Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio==0.7.0) (1.7.0+cu92)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (1.19.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3e-YTe8ZK0Ak","executionInfo":{"status":"ok","timestamp":1624433417015,"user_tz":-480,"elapsed":16,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["import os\n","import numpy as np\n","import pandas as pd"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"5AjX3qrmvj0x","executionInfo":{"status":"ok","timestamp":1624433417016,"user_tz":-480,"elapsed":15,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["# current torch version is 1.7.0+cu101\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import torchaudio\n","\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uUiwwAIP6_o_","executionInfo":{"status":"ok","timestamp":1624433417017,"user_tz":-480,"elapsed":15,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}},"outputId":"b20e65a4-1961-4df7-84bc-7f0732391f94"},"source":["# check if cuda GPU is available, make sure you're using GPU runtime on Google Colab\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device) # you should output \"cuda\""],"execution_count":41,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t9z4qZ8A8HTn"},"source":["## Speech Classification Dataset\n","We will be providing the base dataset that will be used for the first task of the Speech Classification competition."]},{"cell_type":"code","metadata":{"id":"eghj9hRaFWUz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624433621995,"user_tz":-480,"elapsed":204992,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}},"outputId":"9c7401fc-b944-48d5-887f-992e33cc92d8"},"source":["!gdown --id 137fZnrkcm0YbBIBcCoLI2Vgnwq30j1hm\n","!unzip s2_training.zip"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=137fZnrkcm0YbBIBcCoLI2Vgnwq30j1hm\n","To: /content/s2_training.zip\n","81.9MB [00:01, 73.4MB/s]\n","Archive:  s2_training.zip\n","replace challenge_4_training_dataset/CREDITS.txt.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j8Iv0DXlINyH","executionInfo":{"status":"ok","timestamp":1624433621997,"user_tz":-480,"elapsed":16,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["class CustomSpeechDataset(torch.utils.data.Dataset):\n","  def __init__(self, path, typ='train', transforms=None):\n","\n","    assert typ == 'train' or typ == 'test', 'typ must be either \"train\" or \"test\"'\n","\n","    self.typ = typ\n","    self.transforms = transforms\n","    self.targets = []\n","\n","    if self.typ == 'train':\n","      self.class_names = sorted(os.listdir(path))\n","      num_classes = len(self.class_names)\n","\n","      for class_idx, class_name in enumerate(self.class_names):\n","        class_dirx = os.path.join(path, class_name)\n","        wav_list = os.listdir(class_dirx)\n","\n","        for wav_file in wav_list:\n","          self.targets.append({\n","              'filename': wav_file,\n","              'path': os.path.join(class_dirx, wav_file),\n","              'class': class_name\n","          })\n","\n","    if self.typ == 'test':\n","      wav_list = os.listdir(path)\n","      for wav_file in wav_list:\n","        self.targets.append({\n","            'filename': wav_file,\n","            'path': os.path.join(path, wav_file)\n","        })\n","  \n","  def __len__(self):\n","    return len(self.targets)\n","\n","  def __getitem__(self, idx):\n","    if torch.is_tensor(idx):\n","      idx.tolist()\n","\n","    signal, sr = torchaudio.load(self.targets[idx]['path'], normalization=True)\n","    filename = self.targets[idx]['filename']\n","\n","    if self.transforms:\n","      for transform in self.transforms:\n","        signal = transform(signal)\n","\n","    if self.typ == 'train':\n","      clx_name = self.targets[idx]['class']\n","      return filename, signal, sr, clx_name\n","    \n","    elif self.typ == 'test':\n","      return filename, signal, sr"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yCUkL1VKtTH","executionInfo":{"status":"ok","timestamp":1624433621999,"user_tz":-480,"elapsed":15,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["full_dataset = CustomSpeechDataset(path='challenge_4_training_dataset/s2_train_release', typ='train')\n","train_size = int(len(full_dataset)*0.8)\n","valid_size = len(full_dataset) - train_size\n","train_set, valid_set = torch.utils.data.random_split(full_dataset, [train_size, valid_size])"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xtqy62U9K5rv","executionInfo":{"status":"ok","timestamp":1624433622000,"user_tz":-480,"elapsed":15,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["labels = full_dataset.class_names"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uio7QFjHaDoe","executionInfo":{"status":"ok","timestamp":1624433622000,"user_tz":-480,"elapsed":14,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["labels_to_indices = {}\n","for idx, l in enumerate(labels):\n","  labels_to_indices[l] = idx"],"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jrrtzO7KBiq4"},"source":["Let's next look at one example from the training set."]},{"cell_type":"code","metadata":{"id":"IgwfKIIl_P5k","executionInfo":{"status":"ok","timestamp":1624433622001,"user_tz":-480,"elapsed":15,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["# filename, waveform, sample_rate, label_id = train_set[0]"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmL1bsGlALvM","executionInfo":{"status":"ok","timestamp":1624433622001,"user_tz":-480,"elapsed":14,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["# print(\"Shape of waveform: {}\".format(waveform.size()))\n","# print(\"Sample rate of waveform: {}\".format(sample_rate))\n","\n","# Let's plot the waveform using matplotlib\n","# We observe that the main audio activity happens at the later end of the clip\n","# plt.plot(waveform.t().numpy());"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBhtVJs5Ao2s","executionInfo":{"status":"ok","timestamp":1624433622002,"user_tz":-480,"elapsed":15,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["# let's play the audio clip and hear it for ourselves!\n","# ipd.Audio(waveform.numpy(), rate=sample_rate)"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tz-eN4RZTvyW"},"source":["## Constant Sample Lengths\n","In order to insert our features into a model, we have to ensure that the features are of the same size. Below, we see that the sample length varies across the audio clips.\n","\n","Let's pad the audio clips to a maximum sample length of 16000. (16000 sample length is equal to 1 second at 16,000 Hz sampling rate)\n","We will pad audio clips which are less than 1 second in length, with parts of itself."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XeIumefFiZkl","executionInfo":{"status":"ok","timestamp":1624433622684,"user_tz":-480,"elapsed":696,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}},"outputId":"4dc83073-1e57-4b86-817d-d09e2ea32bcd"},"source":["audio_lens = []\n","for i in range(len(train_set)):\n","  audio_lens.append(train_set[i][1].size(1))\n","\n","print('Max Sample Length:', max(audio_lens))\n","print('Min Sample Length:', min(audio_lens))"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Max Sample Length: 16000\n","Min Sample Length: 16000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xLJ4Shh6kN6q","executionInfo":{"status":"ok","timestamp":1624433622685,"user_tz":-480,"elapsed":11,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["class PadAudio(torch.nn.Module):\n","  def __init__(self, req_length = 16000*10):\n","    super().__init__()\n","    self.req_length = req_length\n","\n","  def forward(self, waveform):\n","    while waveform.size(1) < self.req_length:\n","      waveform = torch.cat((waveform, waveform[:, :self.req_length - waveform.size(1)]), axis=1)\n","    return waveform\n","\n","# let's set up a list of transformations we are going to apply to the waveforms\n","transformations = []\n","transformations.append(PadAudio())"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CD5HBKjCA3m"},"source":["## Features\n","In this classification example, instead of using the raw waveform of the audio clips, we will craft handmade audio features known as melspectrograms instead.\n","\n","For an in-depth explanation of what a melspectrogram is, I would highly recommend reading this article [here](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53).\n","\n","In short, a melspectrogram is a way to represent an audio signal’s loudness as it varies over time at different frequencies, while scaled to how humans perceive sound. (We can easily tell the difference between 500 and 1000 Hz, but we can't between 10,000 and 10,500 Hz.)\n","\n","![pic](https://i.ibb.co/WDsqsfb/melspectrogram.png)\n","\n","\n","TorchAudio has an in-built method that can help us with this transformation. We shall then apply log scaling."]},{"cell_type":"code","metadata":{"id":"jlN83ccnJ8It","executionInfo":{"status":"ok","timestamp":1624433622685,"user_tz":-480,"elapsed":10,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["from torchaudio.transforms import MelSpectrogram\n","\n","# We define our own log transformation here\n","class LogMelTransform(torch.nn.Module):\n","\n","    def __init__(self, log_offset = 1e-6):\n","        super().__init__()\n","        self.log_offset = log_offset\n","\n","    def forward(self, melspectrogram):\n","        return torch.log(melspectrogram + self.log_offset)\n","\n","# Let's append these new transformations\n","transformations.append(MelSpectrogram(sample_rate = 16000, n_mels = 128))\n","transformations.append(LogMelTransform())"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E3_rz0d4Khpn"},"source":["## Data Augmentation\n","\n","We will do a simple data augmentation process in order to increase the variations in our dataset.\n","\n","In the audio domain, the augmentation technique known as [SpecAugment](https://arxiv.org/abs/1904.08779) is often used. It makes use of 3 steps:\n","- Time Warp (warps the spectrogram to the left or right)\n","- Frequency Masking (randomly masks a range of frequencies)\n","- Time Masking (randomly masks a range of time)\n","\n","![specaugment pic](https://drive.google.com/uc?export=view&id=1C085-PlXVhjzh4kzCy869VHRGwC3aDHJ)\n","\n","As Time Warp is computationally intensive and does not contribute significant improvement in results, we shall simply use Frequency and Time Masking in this example."]},{"cell_type":"code","metadata":{"id":"omm4z5GsJ-wS","executionInfo":{"status":"ok","timestamp":1624433622686,"user_tz":-480,"elapsed":10,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["from torchaudio.transforms import TimeMasking, FrequencyMasking, Vol, Fade\n","\n","eval_transformations = transformations.copy()\n","\n","# Let's extend the list of transformations with the augmentations\n","transformations.append(TimeMasking(time_mask_param = 50)) # a maximum of 10 time steps will be masked\n","transformations.append(FrequencyMasking(freq_mask_param = 50)) # maximum of 3 freq channels will be masked\n","transformations.append(Vol(gain=1, gain_type=\"db\"))\n","# transformations.append(Fade(fade_in_len=300, fade_out_len=300, fade_shape='exponential'))"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q9hhlhDDMEos"},"source":["## Data Loaders\n","\n","Let's now set up our data loaders so that we can streamline the batch loading of data for our model training later on. "]},{"cell_type":"code","metadata":{"id":"KEJ5I9rNN34D","executionInfo":{"status":"ok","timestamp":1624433622686,"user_tz":-480,"elapsed":10,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["BATCH_SIZE = 8\n","NUM_WORKERS = 4\n","PIN_MEMORY = True if device == 'cuda' else False\n","\n","def train_collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # filename, waveform, sample_rate, label\n","\n","    tensors, targets, filenames = [], [], []\n","\n","    # Gather in lists, and encode labels as indices\n","    for filename, waveform, sample_rate, label in batch:\n","        # apply transformations\n","        for transform in transformations:\n","            waveform = transform(waveform)\n","        waveform = waveform.squeeze().T\n","        tensors += [waveform]\n","        targets += [labels_to_indices[label]]\n","        filenames += [filename]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = torch.stack(tensors)\n","    targets = torch.LongTensor(targets)\n","\n","    return (tensors, targets, filenames)\n","\n","def eval_collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # filename, waveform, sample_rate, label\n","\n","    tensors, targets, filenames = [], [], []\n","\n","    # Gather in lists, and encode labels as indices\n","    for filename, waveform, sample_rate, label in batch:\n","        # apply transformations\n","        for transform in eval_transformations:\n","            waveform = transform(waveform)\n","        waveform = waveform.squeeze().T\n","        tensors += [waveform]\n","        targets += [labels_to_indices[label]]\n","        filenames += [filename]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = torch.stack(tensors)\n","    targets = torch.LongTensor(targets)\n","    filenames += [filename]\n","\n","    return (tensors, targets, filenames)\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_set,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    drop_last=False,\n","    collate_fn=train_collate_fn,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=PIN_MEMORY,\n",")\n","\n","valid_loader = torch.utils.data.DataLoader(\n","    valid_set,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    drop_last=False,\n","    collate_fn=eval_collate_fn,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=PIN_MEMORY,\n",")"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3OWoxpWcM3D4"},"source":["## Setting up the Model\n","\n","In this speech classification example, we will make use of a Long-Short-Term Memory Recurrent Neural Network (LSTM-RNN)."]},{"cell_type":"code","metadata":{"id":"OQWw_iedO1wa","executionInfo":{"status":"ok","timestamp":1624433622687,"user_tz":-480,"elapsed":10,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes, device, classes=None):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","        self.device = device\n","        self.classes = classes\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        batch_size = x.size(0)\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device) \n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device) \n","        \n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # shape = (batch_size, seq_length, hidden_size)\n","        \n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","    def predict(self, x):\n","        '''Predict one label from one sample's features'''\n","        # x: feature from a sample, LxN\n","        #   L is length of sequency\n","        #   N is feature dimension\n","        x = torch.tensor(x[np.newaxis, :], dtype=torch.float32)\n","        x = x.to(self.device)\n","        outputs = self.forward(x)\n","        _, predicted = torch.max(outputs.data, 1)\n","        predicted_index = predicted.item()\n","        return predicted_index"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"_buFizTrXAR8","executionInfo":{"status":"ok","timestamp":1624433622687,"user_tz":-480,"elapsed":10,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["# initialize the model class\n","model = RNN(input_size=128, hidden_size=128, num_layers=2, num_classes=len(labels), device=device, classes=labels).to(device)"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZL3F-89ZXQDU","executionInfo":{"status":"ok","timestamp":1624433622688,"user_tz":-480,"elapsed":10,"user":{"displayName":"Zac Tam Zher Min","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMcuhoTl3lIDTBX-6XcJk_9Sb39L2R117Li2Y-=s64","userId":"05937451928393783963"}}},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","optimizer.zero_grad()\n","num_epochs = 150"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trjdd2wgXqfv","outputId":"11f24e70-37d8-4dee-9dcb-d2a2510d4736"},"source":["for epoch in range(1,num_epochs+1):\n","\n","  # training steps\n","  model.train()\n","  count_correct, count_total = 0, 0\n","  for idx, (features, targets, filenames) in enumerate(train_loader):\n","\n","    features = features.to(device)\n","    targets = targets.to(device)\n","\n","    # forward pass\n","    outputs = model(features)\n","    loss = criterion(outputs, targets)\n","\n","    # backward pass\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # training results\n","    _, argmax = torch.max(outputs, 1)\n","    count_correct += (targets == argmax.squeeze()).sum().item()\n","    count_total += targets.size(0)\n","\n","  train_acc = count_correct / count_total\n","  \n","  # evaluation steps\n","  model.eval()\n","  count_correct, count_total = 0, 0\n","  with torch.no_grad():\n","    for idx, (features, targets, filenames) in enumerate(valid_loader):\n","\n","      features = features.to(device)\n","      targets = targets.to(device)\n","\n","      # forward pass\n","      val_outputs = model(features)\n","      val_loss = criterion(val_outputs, targets)\n","\n","      # validation results\n","      _, argmax = torch.max(val_outputs, 1)\n","      count_correct += (targets == argmax.squeeze()).sum().item()\n","      count_total += targets.size(0)\n","\n","  # print results\n","  valid_acc = count_correct / count_total\n","  print('Epoch [{}/{}], Train loss = {:.4f}, Train accuracy = {:.2f}, Valid loss = {:.4f}, Valid accuracy = {:.2f}' \n","        .format(epoch, num_epochs, loss.item(), 100*train_acc, val_loss.item(), 100*valid_acc))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/150], Train loss = 3.0474, Train accuracy = 8.79, Valid loss = 2.6253, Valid accuracy = 8.31\n","Epoch [2/150], Train loss = 2.9199, Train accuracy = 7.70, Valid loss = 2.4746, Valid accuracy = 6.52\n","Epoch [3/150], Train loss = 2.7135, Train accuracy = 7.44, Valid loss = 2.4886, Valid accuracy = 6.63\n","Epoch [4/150], Train loss = 2.7119, Train accuracy = 7.42, Valid loss = 2.5896, Valid accuracy = 8.65\n","Epoch [5/150], Train loss = 3.2143, Train accuracy = 7.78, Valid loss = 2.5417, Valid accuracy = 10.79\n","Epoch [6/150], Train loss = 3.1433, Train accuracy = 8.90, Valid loss = 2.5509, Valid accuracy = 9.44\n","Epoch [7/150], Train loss = 2.8199, Train accuracy = 10.22, Valid loss = 2.9331, Valid accuracy = 9.78\n","Epoch [8/150], Train loss = 2.7856, Train accuracy = 9.92, Valid loss = 2.5890, Valid accuracy = 11.69\n","Epoch [9/150], Train loss = 2.7300, Train accuracy = 13.76, Valid loss = 2.6516, Valid accuracy = 17.98\n","Epoch [10/150], Train loss = 2.8294, Train accuracy = 15.65, Valid loss = 2.5705, Valid accuracy = 13.82\n","Epoch [11/150], Train loss = 2.4223, Train accuracy = 17.16, Valid loss = 2.9701, Valid accuracy = 16.97\n","Epoch [12/150], Train loss = 3.0120, Train accuracy = 17.75, Valid loss = 2.7975, Valid accuracy = 19.78\n","Epoch [13/150], Train loss = 3.0121, Train accuracy = 18.46, Valid loss = 2.7925, Valid accuracy = 20.90\n","Epoch [14/150], Train loss = 2.6705, Train accuracy = 19.94, Valid loss = 3.2956, Valid accuracy = 20.45\n","Epoch [15/150], Train loss = 2.1290, Train accuracy = 21.40, Valid loss = 3.0356, Valid accuracy = 20.00\n","Epoch [16/150], Train loss = 1.7606, Train accuracy = 21.97, Valid loss = 2.5568, Valid accuracy = 21.12\n","Epoch [17/150], Train loss = 2.6323, Train accuracy = 23.15, Valid loss = 2.9686, Valid accuracy = 21.69\n","Epoch [18/150], Train loss = 2.1240, Train accuracy = 24.16, Valid loss = 3.1533, Valid accuracy = 21.91\n","Epoch [19/150], Train loss = 2.4670, Train accuracy = 22.47, Valid loss = 2.3061, Valid accuracy = 24.49\n","Epoch [20/150], Train loss = 2.5677, Train accuracy = 23.43, Valid loss = 2.4345, Valid accuracy = 23.26\n","Epoch [21/150], Train loss = 2.0722, Train accuracy = 25.39, Valid loss = 2.5725, Valid accuracy = 25.17\n","Epoch [22/150], Train loss = 2.1958, Train accuracy = 27.42, Valid loss = 2.7178, Valid accuracy = 24.83\n","Epoch [23/150], Train loss = 2.5115, Train accuracy = 27.89, Valid loss = 3.0844, Valid accuracy = 23.03\n","Epoch [24/150], Train loss = 2.4588, Train accuracy = 28.20, Valid loss = 2.5244, Valid accuracy = 28.20\n","Epoch [25/150], Train loss = 2.6661, Train accuracy = 31.04, Valid loss = 2.5328, Valid accuracy = 25.28\n","Epoch [26/150], Train loss = 2.8804, Train accuracy = 25.79, Valid loss = 3.1233, Valid accuracy = 20.90\n","Epoch [27/150], Train loss = 2.3028, Train accuracy = 29.10, Valid loss = 2.2186, Valid accuracy = 30.11\n","Epoch [28/150], Train loss = 2.0645, Train accuracy = 29.47, Valid loss = 2.3434, Valid accuracy = 29.78\n","Epoch [29/150], Train loss = 2.1763, Train accuracy = 30.03, Valid loss = 2.6476, Valid accuracy = 29.55\n","Epoch [30/150], Train loss = 1.7900, Train accuracy = 31.01, Valid loss = 2.9411, Valid accuracy = 31.35\n","Epoch [31/150], Train loss = 1.8646, Train accuracy = 32.16, Valid loss = 2.7105, Valid accuracy = 32.70\n","Epoch [32/150], Train loss = 1.7042, Train accuracy = 33.57, Valid loss = 3.1130, Valid accuracy = 35.06\n","Epoch [33/150], Train loss = 1.9196, Train accuracy = 32.81, Valid loss = 2.5103, Valid accuracy = 32.58\n","Epoch [34/150], Train loss = 2.6800, Train accuracy = 34.41, Valid loss = 2.9163, Valid accuracy = 38.31\n","Epoch [35/150], Train loss = 2.0379, Train accuracy = 36.35, Valid loss = 2.1022, Valid accuracy = 32.13\n","Epoch [36/150], Train loss = 2.0557, Train accuracy = 37.50, Valid loss = 2.1927, Valid accuracy = 35.06\n","Epoch [37/150], Train loss = 1.8700, Train accuracy = 34.10, Valid loss = 2.2741, Valid accuracy = 35.73\n","Epoch [38/150], Train loss = 2.8110, Train accuracy = 38.40, Valid loss = 2.8402, Valid accuracy = 36.74\n","Epoch [39/150], Train loss = 2.1959, Train accuracy = 37.28, Valid loss = 2.4313, Valid accuracy = 36.97\n","Epoch [40/150], Train loss = 2.2678, Train accuracy = 36.43, Valid loss = 1.8454, Valid accuracy = 41.12\n","Epoch [41/150], Train loss = 2.2152, Train accuracy = 39.69, Valid loss = 2.4985, Valid accuracy = 37.19\n","Epoch [42/150], Train loss = 2.2369, Train accuracy = 40.81, Valid loss = 2.0139, Valid accuracy = 41.57\n","Epoch [43/150], Train loss = 1.7889, Train accuracy = 40.20, Valid loss = 1.2110, Valid accuracy = 34.49\n","Epoch [44/150], Train loss = 1.9642, Train accuracy = 41.49, Valid loss = 1.9854, Valid accuracy = 36.07\n","Epoch [45/150], Train loss = 1.5243, Train accuracy = 40.96, Valid loss = 1.8086, Valid accuracy = 41.69\n","Epoch [46/150], Train loss = 1.5834, Train accuracy = 44.47, Valid loss = 1.6405, Valid accuracy = 42.58\n","Epoch [47/150], Train loss = 1.7079, Train accuracy = 45.14, Valid loss = 1.9492, Valid accuracy = 40.67\n","Epoch [48/150], Train loss = 1.6282, Train accuracy = 41.60, Valid loss = 1.1617, Valid accuracy = 42.25\n","Epoch [49/150], Train loss = 2.3585, Train accuracy = 44.27, Valid loss = 2.2687, Valid accuracy = 44.49\n","Epoch [50/150], Train loss = 1.4627, Train accuracy = 44.38, Valid loss = 1.1818, Valid accuracy = 42.25\n","Epoch [51/150], Train loss = 1.8635, Train accuracy = 46.12, Valid loss = 2.3633, Valid accuracy = 42.70\n","Epoch [52/150], Train loss = 2.3934, Train accuracy = 41.32, Valid loss = 3.2860, Valid accuracy = 32.81\n","Epoch [53/150], Train loss = 2.4286, Train accuracy = 42.22, Valid loss = 2.0387, Valid accuracy = 29.66\n","Epoch [54/150], Train loss = 1.5767, Train accuracy = 41.63, Valid loss = 2.0231, Valid accuracy = 42.36\n","Epoch [55/150], Train loss = 2.2854, Train accuracy = 44.27, Valid loss = 1.7440, Valid accuracy = 41.91\n","Epoch [56/150], Train loss = 1.3374, Train accuracy = 44.02, Valid loss = 1.6933, Valid accuracy = 39.66\n","Epoch [57/150], Train loss = 1.7279, Train accuracy = 48.65, Valid loss = 1.2664, Valid accuracy = 46.52\n","Epoch [58/150], Train loss = 1.8768, Train accuracy = 44.80, Valid loss = 2.1065, Valid accuracy = 29.44\n","Epoch [59/150], Train loss = 1.9915, Train accuracy = 37.87, Valid loss = 1.7308, Valid accuracy = 44.83\n","Epoch [60/150], Train loss = 1.4260, Train accuracy = 44.35, Valid loss = 4.4642, Valid accuracy = 31.35\n","Epoch [61/150], Train loss = 2.2787, Train accuracy = 38.62, Valid loss = 2.5480, Valid accuracy = 41.01\n","Epoch [62/150], Train loss = 1.5281, Train accuracy = 44.47, Valid loss = 1.9334, Valid accuracy = 42.47\n","Epoch [63/150], Train loss = 1.2246, Train accuracy = 46.24, Valid loss = 1.8751, Valid accuracy = 45.62\n","Epoch [64/150], Train loss = 2.5419, Train accuracy = 42.36, Valid loss = 2.8154, Valid accuracy = 14.94\n","Epoch [65/150], Train loss = 2.1352, Train accuracy = 21.88, Valid loss = 2.9031, Valid accuracy = 23.37\n","Epoch [66/150], Train loss = 2.2321, Train accuracy = 23.09, Valid loss = 2.2080, Valid accuracy = 21.69\n","Epoch [67/150], Train loss = 2.0167, Train accuracy = 22.16, Valid loss = 2.2917, Valid accuracy = 21.01\n","Epoch [68/150], Train loss = 2.2478, Train accuracy = 24.55, Valid loss = 2.5653, Valid accuracy = 22.25\n","Epoch [69/150], Train loss = 2.3679, Train accuracy = 27.13, Valid loss = 2.3224, Valid accuracy = 25.51\n","Epoch [70/150], Train loss = 2.6461, Train accuracy = 30.25, Valid loss = 2.5383, Valid accuracy = 27.75\n","Epoch [71/150], Train loss = 1.6093, Train accuracy = 31.66, Valid loss = 2.4789, Valid accuracy = 30.22\n","Epoch [72/150], Train loss = 2.2789, Train accuracy = 35.03, Valid loss = 3.2251, Valid accuracy = 31.12\n","Epoch [73/150], Train loss = 1.4959, Train accuracy = 34.89, Valid loss = 3.2858, Valid accuracy = 28.54\n","Epoch [74/150], Train loss = 2.5626, Train accuracy = 38.06, Valid loss = 2.8091, Valid accuracy = 28.76\n","Epoch [75/150], Train loss = 2.8443, Train accuracy = 39.41, Valid loss = 2.2466, Valid accuracy = 28.99\n","Epoch [76/150], Train loss = 2.2938, Train accuracy = 38.26, Valid loss = 2.7391, Valid accuracy = 32.81\n","Epoch [77/150], Train loss = 1.8704, Train accuracy = 41.01, Valid loss = 3.1014, Valid accuracy = 36.52\n","Epoch [78/150], Train loss = 2.1237, Train accuracy = 41.43, Valid loss = 3.1987, Valid accuracy = 37.75\n","Epoch [79/150], Train loss = 2.5285, Train accuracy = 31.07, Valid loss = 2.3209, Valid accuracy = 17.08\n","Epoch [80/150], Train loss = 2.5043, Train accuracy = 13.51, Valid loss = 2.3391, Valid accuracy = 9.55\n","Epoch [81/150], Train loss = 3.0326, Train accuracy = 14.30, Valid loss = 2.5298, Valid accuracy = 12.81\n","Epoch [82/150], Train loss = 2.8334, Train accuracy = 15.22, Valid loss = 2.4967, Valid accuracy = 15.17\n","Epoch [83/150], Train loss = 2.9032, Train accuracy = 15.59, Valid loss = 2.4778, Valid accuracy = 14.27\n","Epoch [84/150], Train loss = 3.0720, Train accuracy = 18.12, Valid loss = 2.4170, Valid accuracy = 16.18\n","Epoch [85/150], Train loss = 2.7663, Train accuracy = 18.43, Valid loss = 2.3322, Valid accuracy = 15.06\n","Epoch [86/150], Train loss = 2.4627, Train accuracy = 17.92, Valid loss = 2.3645, Valid accuracy = 14.27\n","Epoch [87/150], Train loss = 2.9835, Train accuracy = 18.20, Valid loss = 2.3147, Valid accuracy = 13.48\n","Epoch [88/150], Train loss = 2.1740, Train accuracy = 17.72, Valid loss = 2.1185, Valid accuracy = 16.74\n","Epoch [89/150], Train loss = 2.4991, Train accuracy = 19.72, Valid loss = 2.1945, Valid accuracy = 18.09\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N-9T9ZEsteo8"},"source":["torch.save(model.state_dict(), 'speech_classification_s2.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUGlO1lFRQQd"},"source":["## Test Set"]},{"cell_type":"code","metadata":{"id":"1vlMBqKgVUjD"},"source":["!gdown --id 1ARTIZB9MdALxltRYtdOZE-6PZ121lEwY\n","!unzip s2_test.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJPXndbLVSRT"},"source":["# Initialise dataset object for test set\n","test_set = CustomSpeechDataset(path='challenge_4_test_dataset', typ='test')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Cqwt6wCV9lg"},"source":["# define test collate function and set up test loader\n","\n","def test_collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # filename, waveform, sample_rate\n","\n","    tensors, filenames = [], []\n","\n","    # Gather in lists\n","    for filename, waveform, sample_rate in batch:\n","        # apply transformations\n","        for transform in eval_transformations:\n","            waveform = transform(waveform)\n","        waveform = waveform.squeeze().T\n","        tensors += [waveform]\n","        filenames += [filename]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = torch.stack(tensors)\n","\n","    return (tensors, filenames)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_set,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    drop_last=False,\n","    collate_fn=test_collate_fn,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=PIN_MEMORY,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wU8vzrvgRUhT"},"source":["# pass test set through the RNN model\n","model.eval()\n","pred_list, filename_list = [], []\n","with torch.no_grad():\n","  for idx, (features, filenames) in enumerate(test_loader):\n","\n","    features = features.to(device)\n","\n","    # forward pass\n","    outputs = model(features)\n","\n","    # validation results\n","    _, argmax = torch.max(outputs, 1)\n","    pred_list += argmax.cpu().tolist()\n","    filename_list += filenames"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeGHrPzIa9_7"},"source":["## Submission of Results\n","Submission csv file should contain only 2 columns for filename and label, in that order. The file should be sorted by filename, and exclude headers. \n","\n","Refer to **sample_submission.csv** for an example."]},{"cell_type":"code","metadata":{"id":"hGPpF_F7QX98"},"source":["result_tuple = list(zip(filename_list, pred_list))\n","submission = pd.DataFrame(result_tuple, columns=['filename', 'pred'])\n","submission = submission.sort_values('filename').reset_index(drop=True)\n","submission['label'] = submission['pred'].apply(lambda x: labels[x])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXO5uUiKZAqZ"},"source":["submission[['filename', 'label']].head()\n","submission[['filename', 'label']].to_csv('submission_s2.csv', header=None, index=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EUmDu88rD1-"},"source":[""],"execution_count":null,"outputs":[]}]}